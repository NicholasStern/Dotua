{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Mode Implementation (High Level)\n",
    "\n",
    "#### In designing our implementation we have focused on \n",
    "\n",
    "1. Expressibility\n",
    "2. Efficiency\n",
    "3. Ease of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressibility\n",
    "\n",
    "Given that Dotua is meant as a partial NumPy replacement, it is essential that users are able to express any mathematical function with Dotua that they could with NumPy.  To this end Dotua offers the following function support with reverse mode automatic differentation\n",
    "\n",
    "- Basic functions: addition, subtraction, multiplication, division, exponentiation, and negation\n",
    "- Trigonometric functions: sine, cosine, and tangent\n",
    "- Inverse trigonometric functions: arcsine, arccosine, arctangent, \n",
    "- Hyperbolic functions: \n",
    "- Logarithms of any base\n",
    "- Roots of arbitrary degree\n",
    "- Logistic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex function demo\n",
    "rad_init = rAutoDiff()\n",
    "\n",
    "vals = [7, 11, 13, 17, 23, 29]\n",
    "a, b, c, d, f, h = rad.create_rscalar(vals)\n",
    "\n",
    "func = op.arccosh(a) / (op.log(c) ** (op.arcsin(b ** -2))) + \\\n",
    "       op.cos((op.tan(h + a) ** op.log(d * h, base=2)) / (op.sin(f) * op.cos(f))\n",
    "\n",
    "func.eval()\n",
    "func.partial(a)\n",
    "func.partial(b)\n",
    "func.partial(c)\n",
    "func.partial(d)\n",
    "func.partial(f)\n",
    "func.partial(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Embed Wolfram Alpha calculation of derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eficiency\n",
    "\n",
    "- Compared with forward mode, fewer operations necessary to compute a gradient of a single function\n",
    "- O(n) vs. O(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ease of Use\n",
    "\n",
    "- Similar to NumPy definitions\n",
    "- Extensive function support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Mode Implementation (Low Level)\n",
    "\n",
    "#### In designing our implementation we have focused on \n",
    "\n",
    "1. rScalar\n",
    "2. rOperator\n",
    "3. rAutoDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rScalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rOperator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rAutoDiff\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
